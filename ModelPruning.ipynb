{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GloC99/diagrams/blob/5CCSACCA/ModelPruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho4RuIGg7r0W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab you will use the tensorflow-model-optimization library to perform model quantisation and pruning on a simple DNN model that classifies hand-written digits from the MNIST dataset."
      ],
      "metadata": {
        "id": "d-AyrrZfQ9y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first install the necessary libraries:"
      ],
      "metadata": {
        "id": "Yn-UUmoISl6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iSh28Uh7uxz",
        "outputId": "cf67ef1e-f3cd-4ee2-c803-2bd718e3e64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Collecting tensorflow-model-optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization) (1.16.0)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's download the MNIST dataset and perform some pre-processing for it:"
      ],
      "metadata": {
        "id": "5MzRfLxUStjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n"
      ],
      "metadata": {
        "id": "Eb7L1pln8Blx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6673ddc-ac0d-40e2-99c6-0dda0b7cb0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step is to build the structure of our DNN model, compile it and start the training process. Note that we will train the model for 5 epochs."
      ],
      "metadata": {
        "id": "3tScrzWyTJhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=5,\n",
        "  validation_split=0.1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCm9g265TKe5",
        "outputId": "1080d3cb-1c38-4182-8020-36b1afb63c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 25s 14ms/step - loss: 0.2719 - accuracy: 0.9245 - val_loss: 0.1124 - val_accuracy: 0.9703\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 19s 11ms/step - loss: 0.1120 - accuracy: 0.9680 - val_loss: 0.0840 - val_accuracy: 0.9772\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 28s 17ms/step - loss: 0.0832 - accuracy: 0.9763 - val_loss: 0.0704 - val_accuracy: 0.9807\n",
            "Epoch 4/5\n",
            "1688/1688 [==============================] - 19s 11ms/step - loss: 0.0669 - accuracy: 0.9805 - val_loss: 0.0654 - val_accuracy: 0.9813\n",
            "Epoch 5/5\n",
            "1688/1688 [==============================] - 21s 12ms/step - loss: 0.0565 - accuracy: 0.9839 - val_loss: 0.0649 - val_accuracy: 0.9817\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7c727058ee30>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model has been trained, let's evaluate its performance on the test data."
      ],
      "metadata": {
        "id": "jHAmRtATTezz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UWrVK1a8PM2",
        "outputId": "80e72c9f-7780-4003-a4e1-1ce3b5b1ee15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0653 - accuracy: 0.9792\n",
            "0.979200005531311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a high-performing model, we can proceed with applying model quantisation to it. For this we need to use the method quantize_model from the tensorflow_model_optimisation library. The following piece of code applies the quantize_model method to our trained model for digit classification and re-compiles the model."
      ],
      "metadata": {
        "id": "jr6U6f4fTmBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lJIpp8KC8c-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how the quantisation has affected the accuracy of our model."
      ],
      "metadata": {
        "id": "HM5G5ASGUmWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, q_aware_accuracy = q_aware_model.evaluate(test_images, test_labels)\n",
        "print(q_aware_accuracy)"
      ],
      "metadata": {
        "id": "1bKpbyyGOvBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0bbd7d1-9ac7-4a5f-c698-bc8ab518ad37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 4.0639 - accuracy: 0.1135\n",
            "0.11349999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the accuracy has gone significanly down. However, this is normal as we did not fine-tune our model after the quantisation. Let's fine-tune the model by training it for 1 epoch with 1000 images."
      ],
      "metadata": {
        "id": "8MHtuNi_UqyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_aware_model.fit(train_images[0:1000], train_labels[0:1000], epochs=1, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aknp7SgELpfr",
        "outputId": "c3ef3a25-64b1-4fc5-da05-93cbe85338fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 1s 10ms/step - loss: 0.0611 - accuracy: 0.9820\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7c0a0ab8e5c0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now evaluate the accuracy of the quantised and fine-tuned model."
      ],
      "metadata": {
        "id": "Td4JeF_RgwA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, q_aware_accuracy = q_aware_model.evaluate(test_images, test_labels)\n",
        "print(q_aware_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVdFqp7NHvHH",
        "outputId": "c1ffc383-6f4c-453b-c5d7-350f346913a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0603 - accuracy: 0.9806\n",
            "0.9805999994277954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the accuracy of the model has improved and is not that different from the original not-quantised model. How cool!\n",
        "\n",
        "Now, let's check the size of the models:"
      ],
      "metadata": {
        "id": "Ki-J2sQ9g2Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the original model (if you haven't pruned it yet)\n",
        "model.save('original_model.h5')\n",
        "\n",
        "# Save the pruned model (after stripping the pruning wrappers)\n",
        "q_aware_model.save('q_aware_model.h5')\n",
        "\n",
        "import os\n",
        "original_size = os.path.getsize('original_model.h5')\n",
        "quantised_size = os.path.getsize('q_aware_model.h5')\n",
        "\n",
        "print(f\"Original model size: {original_size / 1024:.2f} KB\")\n",
        "print(f\"Quantised model size: {quantised_size / 1024:.2f} KB\")\n",
        "print(f\"Size reduction: {(1 - quantised_size / original_size) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTkOwD7ph1fa",
        "outputId": "18c6ff78-480c-419d-8140-c7bc127f6572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model size: 265.97 KB\n",
            "Quantised model size: 282.87 KB\n",
            "Size reduction: -6.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the file size does not decrease much or even increases. This is because we have not yet performed all the necessary steps to complete the quantisation.\n",
        "\n",
        "Let's now use TFLiteConverter to convert our quantised model into TFLite format with default optimisation configurations:"
      ],
      "metadata": {
        "id": "HoLe6hfukIB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1iMZ2S_JVVo",
        "outputId": "bf5f7bf4-f126-42e9-a39d-ffefb41ef02a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now implement the code that would allow us to evaluate the TFLite model. We can not use standart evaluate method for TFLite, as TFLite operates differently from TensorFlow/Keras during inference. More specifically, a TFLite model is loaded and executed using a TFLite interpreter, which operates on a lightweight, deployment-friendly runtime designed for inference only. It does not have built-in functions for model evaluation, like model.evaluate in Keras."
      ],
      "metadata": {
        "id": "KQe4S_D_lBsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "wZ206g0sJ694"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's evaluate the TFLite model we have with the implemented evaluation method."
      ],
      "metadata": {
        "id": "b6cWWndNnJ4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "print('Quant TF test accuracy:', q_aware_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJsu5gGIKDqB",
        "outputId": "fa5c6622-eddb-4ac3-d9c1-02c520e89b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Quant TFLite test_accuracy: 0.9806\n",
            "Quant TF test accuracy: 0.9805999994277954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's save the original and quantised models and see how different are their sizes."
      ],
      "metadata": {
        "id": "rkJbDpDqntiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "\n",
        "# Create float TFLite model.\n",
        "float_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "float_tflite_model = float_converter.convert()\n",
        "\n",
        "# Measure sizes of models.\n",
        "_, float_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(float_file, 'wb') as f:\n",
        "  f.write(float_tflite_model)\n",
        "\n",
        "print(\"Float model in Mb:\", os.path.getsize(float_file) / float(2**20))\n",
        "print(\"Quantized model in Mb:\", os.path.getsize(quant_file) / float(2**20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5es9PiJKKIo3",
        "outputId": "8fb5536c-9610-430d-b1e5-fc6f67da1bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model in Mb: 0.08073043823242188\n",
            "Quantized model in Mb: 0.023681640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model size reduces by around 4 times, without any significant loss in accuracy. This concludes the part of this lab which focuses on model quantisation.\n",
        "\n",
        "\n",
        "Let's know use the tensorflow-model-optimization to perform model pruning. We first define a pruning schedule. Here are the parameters we use:\n",
        "\n",
        "**initial_sparsity**: The fraction of weights set to zero at the start of pruning.\n",
        "\n",
        "**final_sparsity**: The fraction of weights to be zeroed out by the end of pruning.\n",
        "\n",
        "**begin_step**: The training step at which pruning begins.\n",
        "\n",
        "**end_step**: The training step at which pruning ends.\n",
        "\n",
        "**frequency**: The interval (in steps) at which pruning is applied."
      ],
      "metadata": {
        "id": "NQu1NE87oiT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pruning schedule\n",
        "pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
        "    initial_sparsity=0.0,  # Start with no sparsity\n",
        "    final_sparsity=0.5,    # Target 50% sparsity\n",
        "    begin_step=0,\n",
        "    end_step=2000,  # Adjust this value depending on the total number of steps\n",
        "    frequency=100   # Apply pruning every 100 steps\n",
        ")"
      ],
      "metadata": {
        "id": "KFXC83cPNs2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now wrap the model to include pruning logic and re-compile the model."
      ],
      "metadata": {
        "id": "mvIeHc9IqI3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply pruning to the model\n",
        "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n",
        "\n",
        "# Compile the pruned model\n",
        "pruned_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "WWYEvgDMP2aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a pruning-specific callback to update the pruning step during training. We then fine-tune the pruned model, gradually increasing sparsity as per the schedule while monitoring accuracy."
      ],
      "metadata": {
        "id": "7-NPRR89qU62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pruning callback\n",
        "pruning_callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep()\n",
        "]\n",
        "\n",
        "# Fine-tune the pruned model with the pruning callback\n",
        "pruned_model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=5,\n",
        "    validation_split=0.1,\n",
        "    callbacks=pruning_callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B0QmaEZqVvw",
        "outputId": "77fe04a6-d989-447c-c7f7-1a7a40a8ce28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 22s 11ms/step - loss: 0.0518 - accuracy: 0.9850 - val_loss: 0.0589 - val_accuracy: 0.9845\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 18s 11ms/step - loss: 0.0448 - accuracy: 0.9873 - val_loss: 0.0604 - val_accuracy: 0.9853\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 22s 13ms/step - loss: 0.0412 - accuracy: 0.9881 - val_loss: 0.0578 - val_accuracy: 0.9848\n",
            "Epoch 4/5\n",
            "1688/1688 [==============================] - 19s 11ms/step - loss: 0.0377 - accuracy: 0.9894 - val_loss: 0.0596 - val_accuracy: 0.9860\n",
            "Epoch 5/5\n",
            "1688/1688 [==============================] - 19s 11ms/step - loss: 0.0357 - accuracy: 0.9893 - val_loss: 0.0607 - val_accuracy: 0.9848\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7c0a36f208e0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then strip the pruning wrappers, save the final pruned model, compile it and evaluate its performance."
      ],
      "metadata": {
        "id": "I4BrX-qPqpqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip the pruning wrappers and save the final pruned model\n",
        "final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "\n",
        "final_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Now you can proceed with evaluation\n",
        "pruned_accuracy = final_model.evaluate(test_images, test_labels, verbose=0)[1]\n",
        "\n",
        "print(f\"Pruned model accuracy: {pruned_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S25gya4wR1ug",
        "outputId": "cc5314b5-aacd-407c-c5cc-d12fbcdedf55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model accuracy: 98.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model's accuracy remains almost the same. Now, let's see how the size of the model changes."
      ],
      "metadata": {
        "id": "hkgx0Br8qzmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the original model (if you haven't pruned it yet)\n",
        "model.save('original_model.h5')\n",
        "\n",
        "# Save the pruned model (after stripping the pruning wrappers)\n",
        "final_model.save('pruned_model.h5')\n",
        "\n",
        "import os\n",
        "original_size = os.path.getsize('original_model.h5')\n",
        "pruned_size = os.path.getsize('pruned_model.h5')\n",
        "\n",
        "print(f\"Original model size: {original_size / 1024:.2f} KB\")\n",
        "print(f\"Pruned model size: {pruned_size / 1024:.2f} KB\")\n",
        "print(f\"Size reduction: {(1 - pruned_size / original_size) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6KlNJJ0SBpN",
        "outputId": "973f90d0-6d8e-4bb7-c32c-83e00cdee368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model size: 265.97 KB\n",
            "Pruned model size: 98.03 KB\n",
            "Size reduction: 63.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the size reduces by around 65% without any significant loss in accuracy."
      ],
      "metadata": {
        "id": "_n9r_DuZrBqd"
      }
    }
  ]
}